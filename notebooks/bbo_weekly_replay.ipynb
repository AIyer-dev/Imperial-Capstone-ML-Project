{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# BBO Weekly Replay Notebook\n\nThis notebook **replays your optimisation week-by-week**.\n\nFor each week *t*, it:\n1. Fits a GP surrogate for each function using data up to week *t*\n2. Proposes a candidate query for week *t+1* using EI/UCB\n3. Reports the **predicted mean/std** at that proposed point\n4. Also generates a **heuristic** trust-region / rollback / freeze suggestion\n\nIt outputs a tidy table you can export to CSV and use in your report.\n\n## Notes\n- This is meant as a *transparent reconstruction tool*.\n- Your actual weekly decisions in chat sometimes used heuristics, so comparing GP vs heuristic is useful.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import re\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n\nnp.set_printoptions(suppress=True, precision=6)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Load & parse history\n\nWe support two formats:\n1. Explicit labels: `F3: 0.930000-0.620000-0.880000`\n2. Plain vectors: 8 lines per week, in order F1..F8, repeating.\n\nOutputs are assumed scalar; if a vector appears, we take the first value.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "FUNC_NAMES = [f\"F{i}\" for i in range(1, 9)]\nDIMS = {\"F1\":2,\"F2\":2,\"F3\":3,\"F4\":4,\"F5\":4,\"F6\":5,\"F7\":6,\"F8\":8}\n\nvec_pat = re.compile(r\"^[0-9]\\.[0-9]{6}(?:-[0-9]\\.[0-9]{6})+$\")\nfx_pat  = re.compile(r\"\\b(F[1-8])\\b\\s*[:=]?\\s*([0-9]\\.[0-9]{6}(?:-[0-9]\\.[0-9]{6}){1,7})\")\n\ndef parse_vector(s: str) -> np.ndarray:\n    parts = [p for p in s.strip().split('-') if p]\n    return np.array([float(p) for p in parts], dtype=float)\n\ndef parse_file_to_sequence(text: str):\n    \"\"\"Return list of (fx, vector_string) in chronological order.\"\"\"\n    matches = list(fx_pat.finditer(text))\n    if matches:\n        return [(m.group(1), m.group(2)) for m in matches]\n\n    # fallback to plain vectors: assume order is F1..F8 repeating\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    vec_lines = [ln for ln in lines if vec_pat.match(ln)]\n    seq = []\n    for i, ln in enumerate(vec_lines):\n        fx = FUNC_NAMES[i % 8]\n        seq.append((fx, ln))\n    return seq\n\ndef load_weekly_history(inputs_path: str, outputs_path: str):\n    with open(inputs_path, \"r\", encoding=\"utf-8\") as f:\n        inp_text = f.read()\n    with open(outputs_path, \"r\", encoding=\"utf-8\") as f:\n        out_text = f.read()\n\n    inp_seq = parse_file_to_sequence(inp_text)\n    out_seq = parse_file_to_sequence(out_text)\n\n    # align length (pairs per function evaluation)\n    n = min(len(inp_seq), len(out_seq))\n    inp_seq = inp_seq[:n]\n    out_seq = out_seq[:n]\n\n    # infer number of weeks: every week has 8 functions\n    if n % 8 != 0:\n        print(f\"Warning: total records {n} not multiple of 8; truncating to full weeks.\")\n        n = (n // 8) * 8\n        inp_seq = inp_seq[:n]\n        out_seq = out_seq[:n]\n\n    n_weeks = n // 8\n\n    # Build per-week dict: week -> {fx: (x, y)}\n    weeks = []\n    idx = 0\n    for w in range(n_weeks):\n        week_data = {}\n        for j in range(8):\n            fx_i, x_str = inp_seq[idx]\n            fx_o, y_str = out_seq[idx]\n            # If labels exist, ensure they match; if not, trust sequence order\n            fx = fx_i\n            x = parse_vector(x_str)\n            y = float(parse_vector(y_str)[0])\n            week_data[fx] = (x, y)\n            idx += 1\n        weeks.append(week_data)\n    return weeks\n\n# Example:\n# weeks = load_weekly_history(\"inputs.txt\", \"outputs.txt\")\n# print(len(weeks), \"weeks loaded\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) GP surrogate + acquisitions (EI/UCB)\nWe fit a GP on data up to week *t* and propose a point for week *t+1* by maximising EI (or UCB) over random candidates.\n\nTo avoid extra dependencies, EI uses an erf-based Normal approximation.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def fit_gp(X: np.ndarray, y: np.ndarray, seed: int = 0) -> GaussianProcessRegressor:\n    y_mean = y.mean() if len(y) else 0.0\n    y_std = y.std() if len(y) > 1 else 1.0\n    y_std = y_std if y_std > 1e-8 else 1.0\n    y_norm = (y - y_mean) / y_std\n\n    kernel = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(length_scale=np.ones(X.shape[1]), nu=2.5)              + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-8, 1e-1))\n\n    gp = GaussianProcessRegressor(\n        kernel=kernel,\n        alpha=0.0,\n        normalize_y=False,\n        n_restarts_optimizer=5,\n        random_state=seed\n    )\n    gp.fit(X, y_norm)\n    gp._y_mean = y_mean\n    gp._y_std = y_std\n    return gp\n\ndef gp_predict(gp: GaussianProcessRegressor, Xcand: np.ndarray):\n    mu_norm, std_norm = gp.predict(Xcand, return_std=True)\n    mu = mu_norm * gp._y_std + gp._y_mean\n    std = std_norm * gp._y_std\n    return mu, std\n\ndef ucb(mu: np.ndarray, std: np.ndarray, beta: float = 2.0):\n    return mu + beta * std\n\ndef expected_improvement(mu: np.ndarray, std: np.ndarray, best: float, xi: float = 0.01):\n    sqrt2 = np.sqrt(2.0)\n    Z = np.divide(mu - best - xi, std, out=np.zeros_like(mu), where=std > 1e-12)\n    pdf = (1.0 / np.sqrt(2*np.pi)) * np.exp(-0.5 * Z**2)\n    cdf = 0.5 * (1.0 + np.erf(Z / sqrt2))\n    imp = mu - best - xi\n    ei = imp * cdf + std * pdf\n    ei[std <= 1e-12] = 0.0\n    return ei\n\ndef propose_next_gp(X: np.ndarray, y: np.ndarray, acquisition: str = \"EI\",\n                    n_candidates: int = 20000, seed: int = 0, beta: float = 2.0, xi: float = 0.01):\n    d = X.shape[1]\n    rng = np.random.default_rng(seed)\n\n    if len(y) < max(3, d+1):\n        x_next = rng.random(d)\n        return x_next, {\"reason\":\"too_few_points_random\", \"pred_mu\": np.nan, \"pred_std\": np.nan}\n\n    gp = fit_gp(X, y, seed=seed)\n    Xcand = rng.random((n_candidates, d))\n    mu, std = gp_predict(gp, Xcand)\n    best = float(np.max(y))\n\n    if acquisition.upper() == \"EI\":\n        score = expected_improvement(mu, std, best=best, xi=xi)\n    else:\n        score = ucb(mu, std, beta=beta)\n\n    idx = int(np.argmax(score))\n    x_star = Xcand[idx]\n    # Predict at the chosen point\n    mu_star, std_star = gp_predict(gp, x_star.reshape(1, -1))\n    return x_star, {\"pred_mu\": float(mu_star[0]), \"pred_std\": float(std_star[0]), \"best_so_far\": best, \"acq\": acquisition}\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Heuristic proposer\nThis is a lightweight approximation of the decision rules we often used:\n- If the function is flat: freeze at 0.5\n- If last point regressed a lot: rollback to best\n- Else: small perturbation around best (trust region)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def propose_next_heuristic(X: np.ndarray, y: np.ndarray, step: float = 0.01, seed: int = 0):\n    d = X.shape[1]\n    rng = np.random.default_rng(seed)\n\n    if len(y) == 0:\n        return rng.random(d), {\"reason\":\"no_data_random\"}\n\n    y_range = float(np.max(y) - np.min(y))\n    best_idx = int(np.argmax(y))\n    x_best = X[best_idx].copy()\n    y_best = float(y[best_idx])\n\n    if y_range < 1e-6:\n        return np.full(d, 0.5), {\"reason\":\"flat_freeze_center\", \"best\": y_best}\n\n    if len(y) >= 2 and (y_best - float(y[-1])) > 0.25 * max(1e-6, y_range):\n        return x_best, {\"reason\":\"rollback_to_best\", \"best\": y_best}\n\n    x = x_best.copy()\n    k = 1 if d <= 3 else 2\n    dims = rng.choice(d, size=k, replace=False)\n    for j in dims:\n        x[j] = np.clip(x[j] + rng.choice([-1.0, 1.0]) * step, 0.0, 0.999999)\n    return x, {\"reason\":\"trust_region_perturb\", \"best\": y_best}\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Weekly replay table\nFor each week *t*, we train on weeks `1..t` and propose a point for week `t+1`.\n\nYou can choose:\n- `METHOD = 'GP'` or `METHOD = 'HEURISTIC'`\n- or run both and compare.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def fmt_vec(x: np.ndarray) -> str:\n    # Keep values in [0, 0.999999] and print to 6dp\n    x = np.clip(x, 0.0, 0.999999)\n    return \"-\".join([f\"{v:.6f}\" for v in x])\n\ndef build_replay_table(weeks, acquisition=\"EI\", n_candidates=20000, beta=2.0, xi=0.01):\n    rows = []\n    n_weeks = len(weeks)\n\n    # Pre-accumulate data per function\n    X_hist = {fx: [] for fx in FUNC_NAMES}\n    y_hist = {fx: [] for fx in FUNC_NAMES}\n\n    for t in range(n_weeks):  # week index 0..n_weeks-1\n        # add week t observations\n        for fx in FUNC_NAMES:\n            x, y = weeks[t][fx]\n            X_hist[fx].append(x)\n            y_hist[fx].append(y)\n\n        # propose for next week (t+1) if exists\n        if t == n_weeks - 1:\n            break\n\n        for fx in FUNC_NAMES:\n            X = np.vstack(X_hist[fx])\n            y = np.array(y_hist[fx], dtype=float)\n\n            d = DIMS[fx]\n            seed = 1000*t + (hash(fx) % 997)\n\n            # GP propose\n            x_gp, info_gp = propose_next_gp(\n                X, y,\n                acquisition=acquisition,\n                n_candidates=n_candidates,\n                seed=seed,\n                beta=beta,\n                xi=xi\n            )\n\n            # heuristic propose\n            step = 0.005 if d >= 4 else 0.01\n            x_h, info_h = propose_next_heuristic(X, y, step=step, seed=seed+7)\n\n            rows.append({\n                \"train_up_to_week\": t+1,\n                \"predict_for_week\": t+2,\n                \"function\": fx,\n                \"best_so_far\": float(np.max(y)),\n                \"GP_suggestion\": fmt_vec(x_gp),\n                \"GP_pred_mu\": info_gp.get(\"pred_mu\", np.nan),\n                \"GP_pred_std\": info_gp.get(\"pred_std\", np.nan),\n                \"Heuristic_suggestion\": fmt_vec(x_h),\n                \"Heuristic_reason\": info_h.get(\"reason\",\"\")\n            })\n\n    return pd.DataFrame(rows)\n\n# --- Run ---\nINPUTS_PATH = \"inputs.txt\"\nOUTPUTS_PATH = \"outputs.txt\"\nweeks = load_weekly_history(INPUTS_PATH, OUTPUTS_PATH)\n\nreplay_df = build_replay_table(weeks, acquisition=\"EI\", n_candidates=25000, xi=0.01)\nreplay_df.head(16)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Compare suggestions to what you actually submitted\nThis helps you audit how close the GP/heuristic recommendations were to your real submissions.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def actual_submissions_table(weeks):\n    rows = []\n    for t, wk in enumerate(weeks, start=1):\n        for fx in FUNC_NAMES:\n            x, y = wk[fx]\n            rows.append({\n                \"week\": t,\n                \"function\": fx,\n                \"submitted_x\": fmt_vec(x),\n                \"observed_y\": float(y)\n            })\n    return pd.DataFrame(rows)\n\nactual_df = actual_submissions_table(weeks)\n\n# Merge: for week t, compare to model suggestion made after training up to t-1.\n# replay_df predicts_for_week = t, so join on that.\nmerged = actual_df.merge(\n    replay_df,\n    left_on=[\"week\",\"function\"],\n    right_on=[\"predict_for_week\",\"function\"],\n    how=\"left\"\n)\n\nmerged.head(16)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Export to CSV (optional)\nYou can export the replay table and merged comparison to CSV.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# replay_df.to_csv(\"replay_suggestions.csv\", index=False)\n# merged.to_csv(\"replay_vs_actual.csv\", index=False)\n\nprint(\"Rows in replay_df:\", len(replay_df))\nprint(\"Rows in merged:\", len(merged))\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}