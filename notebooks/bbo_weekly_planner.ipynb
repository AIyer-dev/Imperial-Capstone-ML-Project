{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# BBO Weekly Planner (GP + Acquisition + Heuristics)\n\nThis notebook gives you a **reproducible** way to generate *next-week candidate queries* for the Imperial BBO capstone.\n\n## Important honesty note\nIn our chat, many week-to-week suggestions were **heuristic + pattern-based** (trust-region rollbacks, freezing plateaus, monotonic pushes), not the output of a single formal predictor.\n\nTo make this useful (and to document the methodology), this notebook implements:\n1) **Gaussian Process (GP) surrogate modelling** per function (scikit-learn)\n2) **Acquisition functions** (Expected Improvement, UCB)\n3) A **heuristic mode** that mirrors the capstone-style reasoning.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import re\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n\nnp.set_printoptions(suppress=True, precision=6)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Parsing your capstone history files\n\nThis parser handles common formats:\n- `F3: 0.930000-0.620000-0.880000`\n- or plain vector lines (8 lines per week)\n\nAdjust `extract_fx_lines()` if your files differ.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "FUNC_NAMES = [f\"F{i}\" for i in range(1, 9)]\nDIMS = {\"F1\":2,\"F2\":2,\"F3\":3,\"F4\":4,\"F5\":4,\"F6\":5,\"F7\":6,\"F8\":8}\n\ndef parse_vector(s: str) -> np.ndarray:\n    s = s.strip()\n    parts = [p for p in s.split(\"-\") if p]\n    return np.array([float(p) for p in parts], dtype=float)\n\ndef extract_fx_lines(text: str):\n    out = {k: [] for k in FUNC_NAMES}\n    pat = re.compile(r\"\\b(F[1-8])\\b\\s*[:=]?\\s*([0-9]\\.[0-9]{6}(?:-[0-9]\\.[0-9]{6}){1,7})\")\n    for m in pat.finditer(text):\n        out[m.group(1)].append(m.group(2))\n    return out\n\ndef load_history(inputs_path: str, outputs_path: str):\n    with open(inputs_path, \"r\", encoding=\"utf-8\") as f:\n        inp_text = f.read()\n    with open(outputs_path, \"r\", encoding=\"utf-8\") as f:\n        out_text = f.read()\n\n    inp_map = extract_fx_lines(inp_text)\n    out_map = extract_fx_lines(out_text)\n\n    # Fallback: assume pure vector lines repeating F1..F8\n    if all(len(v) == 0 for v in inp_map.values()):\n        inp_lines = [ln.strip() for ln in inp_text.splitlines() if ln.strip()]\n        vec_lines = [ln for ln in inp_lines if re.match(r\"^[0-9]\\.[0-9]{6}(?:-[0-9]\\.[0-9]{6})+$\", ln)]\n        for i, ln in enumerate(vec_lines):\n            fx = FUNC_NAMES[i % 8]\n            inp_map[fx].append(ln)\n\n    if all(len(v) == 0 for v in out_map.values()):\n        out_lines = [ln.strip() for ln in out_text.splitlines() if ln.strip()]\n        vec_lines = [ln for ln in out_lines if re.match(r\"^[0-9]\\.[0-9]{6}(?:-[0-9]\\.[0-9]{6})+$\", ln)]\n        for i, ln in enumerate(vec_lines):\n            fx = FUNC_NAMES[i % 8]\n            out_map[fx].append(ln)\n\n    history = {}\n    for fx in FUNC_NAMES:\n        Xs = [parse_vector(v) for v in inp_map[fx]]\n        ys = []\n        for raw in out_map[fx]:\n            vec = parse_vector(raw)\n            ys.append(float(vec[0]))\n        n = min(len(Xs), len(ys))\n        if n == 0:\n            history[fx] = (np.zeros((0, DIMS[fx])), np.zeros((0,)))\n        else:\n            X = np.vstack(Xs[:n])\n            y = np.array(ys[:n], dtype=float)\n            history[fx] = (X, y)\n    return history\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. GP surrogate + acquisition (EI/UCB)\n\nWe fit a GP per function and choose the next point by maximising an acquisition function.\nOptimisation is done via random search over the unit hypercube.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def fit_gp(X: np.ndarray, y: np.ndarray, seed: int = 0) -> GaussianProcessRegressor:\n    y_mean = y.mean() if len(y) else 0.0\n    y_std = y.std() if len(y) > 1 else 1.0\n    y_std = y_std if y_std > 1e-8 else 1.0\n    y_norm = (y - y_mean) / y_std\n\n    kernel = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(length_scale=np.ones(X.shape[1]), nu=2.5)              + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-8, 1e-1))\n\n    gp = GaussianProcessRegressor(\n        kernel=kernel,\n        alpha=0.0,\n        normalize_y=False,\n        n_restarts_optimizer=5,\n        random_state=seed\n    )\n    gp.fit(X, y_norm)\n    gp._y_mean = y_mean\n    gp._y_std = y_std\n    return gp\n\ndef gp_predict(gp: GaussianProcessRegressor, Xcand: np.ndarray):\n    mu_norm, std_norm = gp.predict(Xcand, return_std=True)\n    mu = mu_norm * gp._y_std + gp._y_mean\n    std = std_norm * gp._y_std\n    return mu, std\n\ndef ucb(mu: np.ndarray, std: np.ndarray, beta: float = 2.0):\n    return mu + beta * std\n\ndef expected_improvement(mu: np.ndarray, std: np.ndarray, best: float, xi: float = 0.01):\n    # Avoid SciPy dependency: approximate normal CDF/PDF via error function.\n    # Good enough for ranking candidates in random search.\n    import numpy as np\n    sqrt2 = np.sqrt(2.0)\n    Z = np.divide(mu - best - xi, std, out=np.zeros_like(mu), where=std > 1e-12)\n    pdf = (1.0 / np.sqrt(2*np.pi)) * np.exp(-0.5 * Z**2)\n    cdf = 0.5 * (1.0 + np.erf(Z / sqrt2))\n    imp = mu - best - xi\n    ei = imp * cdf + std * pdf\n    ei[std <= 1e-12] = 0.0\n    return ei\n\ndef propose_next_gp(X: np.ndarray, y: np.ndarray, acquisition: str = \"EI\",\n                    n_candidates: int = 15000, seed: int = 0, beta: float = 2.0, xi: float = 0.01):\n    d = X.shape[1]\n    rng = np.random.default_rng(seed)\n\n    if len(y) < max(3, d+1):\n        x_next = rng.random(d)\n        return x_next, {\"reason\":\"too_few_points_random\"}\n\n    gp = fit_gp(X, y, seed=seed)\n    Xcand = rng.random((n_candidates, d))\n    mu, std = gp_predict(gp, Xcand)\n    best = float(np.max(y))\n\n    if acquisition.upper() == \"EI\":\n        score = expected_improvement(mu, std, best=best, xi=xi)\n    else:\n        score = ucb(mu, std, beta=beta)\n\n    idx = int(np.argmax(score))\n    return Xcand[idx], {\"pred_mu\": float(mu[idx]), \"pred_std\": float(std[idx]), \"best_so_far\": best, \"acq\": acquisition}\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Heuristic proposer (mirrors our weekly reasoning)\n\n- Freeze flat/plateau functions\n- Roll back after large regressions\n- Otherwise do a small trust-region perturbation around the best point\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def propose_next_heuristic(X: np.ndarray, y: np.ndarray, step: float = 0.01, seed: int = 0):\n    d = X.shape[1]\n    rng = np.random.default_rng(seed)\n\n    if len(y) == 0:\n        return rng.random(d), {\"reason\":\"no_data_random\"}\n\n    y_range = float(np.max(y) - np.min(y))\n    best_idx = int(np.argmax(y))\n    x_best = X[best_idx].copy()\n    y_best = float(y[best_idx])\n\n    if y_range < 1e-6:\n        return np.full(d, 0.5), {\"reason\":\"flat_freeze_center\", \"best\": y_best}\n\n    if len(y) >= 2 and (y_best - float(y[-1])) > 0.25 * max(1e-6, y_range):\n        return x_best, {\"reason\":\"rollback_to_best\", \"best\": y_best}\n\n    x = x_best.copy()\n    k = 1 if d <= 3 else 2\n    dims = rng.choice(d, size=k, replace=False)\n    for j in dims:\n        x[j] = np.clip(x[j] + rng.choice([-1.0, 1.0]) * step, 0.0, 0.999999)\n    return x, {\"reason\":\"trust_region_perturb\", \"best\": y_best}\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Generate next-week suggestions\n\nUpdate these paths to your current `inputs.txt` and `outputs.txt` and run.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "INPUTS_PATH = \"inputs.txt\"\nOUTPUTS_PATH = \"outputs.txt\"\n\nhistory = load_history(INPUTS_PATH, OUTPUTS_PATH)\n\ndef fmt_vec(x):\n    return \"-\".join([f\"{v:.6f}\" if v < 1 else \"0.999999\" for v in x])\n\nrows = []\nfor fx in FUNC_NAMES:\n    X, y = history[fx]\n    if X.shape[0] == 0:\n        X = np.zeros((0, DIMS[fx]))\n        y = np.zeros((0,))\n\n    x_gp, info_gp = propose_next_gp(X, y, acquisition=\"EI\", n_candidates=20000, seed=42+hash(fx)%1000, xi=0.01)\n    step = 0.005 if DIMS[fx] >= 4 else 0.01\n    x_h, info_h = propose_next_heuristic(X, y, step=step, seed=7+hash(fx)%1000)\n\n    rows.append({\n        \"Function\": fx,\n        \"n_points\": int(len(y)),\n        \"best_so_far\": float(np.max(y)) if len(y) else np.nan,\n        \"GP_next\": fmt_vec(x_gp),\n        \"GP_pred_mu\": info_gp.get(\"pred_mu\", np.nan),\n        \"GP_pred_std\": info_gp.get(\"pred_std\", np.nan),\n        \"Heuristic_next\": fmt_vec(x_h),\n        \"Heuristic_reason\": info_h.get(\"reason\",\"\")\n    })\n\ndf = pd.DataFrame(rows)\ndf\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Practical tips\n\n- For **high dimensions (F7/F8)**, GP predictions can be noisy with small data. Consider using the heuristic mode or lock-in best-known points.\n- For final rounds, a defensible policy is often: **submit the best observed point** (confirmation > speculation).\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}